# üöÄ H∆Ø·ªöNG D·∫™N DEPLOY H·ªÜ TH·ªêNG 2-CLUSTER

## Ki·∫øn tr√∫c h·ªá th·ªëng

**Cluster 1 (Application Cluster):**

- 1 master + 2 workers (role=app)
- Ch·∫°y: Longhorn, Databases, Application services
  **Cluster 2 (Monitoring Cluster):**
- 1 master + 1 worker (role=monitoring)
- Ch·∫°y: Longhorn, Prometheus, Grafana, Kube-state-metrics
- Cross-cluster metrics collection t·ª´ Cluster 1

## Y√™u c·∫ßu

- ‚úÖ 2 K3s clusters ƒë√£ c√†i ƒë·∫∑t
- ‚úÖ kubectl c√≥ quy·ªÅn truy c·∫≠p c·∫£ 2 clusters
- ‚úÖ Docker images ƒë√£ push l√™n Docker Hub
- ‚úÖ 2 clusters c√πng subnet

---

## üì¶ CLUSTER 1: APPLICATION CLUSTER

### 1. C√†i ƒë·∫∑t Longhorn Storage

```bash
# C√†i ƒë·∫∑t Longhorn tr√™n Cluster 1
kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.10.1/deploy/longhorn.yaml
# ƒê·ª£i Longhorn ready
kubectl get pods -n longhorn-system --watch
# (Optional) Expose Longhorn UI
kubectl patch svc longhorn-frontend -n longhorn-system -p '{"spec":{"type":"NodePort","ports":[{"port":80,"targetPort":8000,"nodePort":30880}]}}'
```

**Longhorn UI**: http://192.168.40.121:30880

### 2. Label Nodes & Create Namespaces

```bash
# Label nodes theo role
kubectl label nodes worker1 role=app
kubectl label nodes worker2 role=app
# T·∫°o namespaces
kubectl create namespace databases
kubectl create namespace prod
kubectl create namespace monitoring
# Verify
kubectl get nodes --show-labels | grep role
kubectl get namespaces
```

### 3. Deploy Databases

```bash
kubectl apply -f deployment/databases/databases.yaml
# ƒê·ª£i databases ready
kubectl get pods -n databases --watch
# Ch·ªù: postgres-0 1/1 Running, user-db-0 1/1 Running
```

### 4. Deploy Application Services

````bash
# Deploy config v√† backend services
kubectl apply -f deployment/prod/config-prod.yaml
kubectl apply -f deployment/prod/application/todo-service-prod.yaml
kubectl apply -f deployment/prod/application/user-service-prod.yaml
kubectl apply -f deployment/prod/application/stats-service-prod.yaml
# Deploy frontend
kubectl apply -f deployment/prod/frontend-prod.yaml
# Ki·ªÉm tra
kubectl get pods -n prod --watch```
### 5. Expose Metrics Endpoints
```bash
# Deploy kube-state-metrics tr√™n Cluster 1
kubectl apply -f deployment/prod/kube-state-metrics.yaml
# Verify
kubectl get svc -n prod | grep metrics
kubectl get svc -n monitoring
````

**C√†i ƒë·∫∑t VIP (Virtual IP) ƒë·ªÉ c√°c services c√≥ 1 ip chung**

```bash
kubectl apply -f deployment/prod/rbac.yaml
kubectl apply -f deployment/prod/kube-vip-daemonset.yaml
```

**C√°c services ƒë∆∞·ª£c ƒë·ªïi th√†nh LoadBalacer v√† c√≥ LoadbalancerIP r·ªìi -> xem chi ti·∫øt trong c√°c file yaml c·ªßa services**
**Truy c·∫≠p: 192.168.40.205**

---

### C·∫•u h√¨nh HPA (Horizontal Pod Autoscaler)

**C√†i metrics server**

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

kubectl patch deployment metrics-server -n kube-system --type='json' -p='[{"op":"add","path":"/spec/template/spec/containers/0/args/-","value":"--kubelet-insecure-tls"}]'

kubectl get deployment metrics-server -n kube-system
```

**√°p d·ª•ng hpa controller**

```bash
kubectl deployment/prod/application/hpa.yaml
kubectl get hpa -n prod
```

## üì¶ CLUSTER 2: MONITORING CLUSTER

### 1. C√†i ƒë·∫∑t Longhorn Storage

````bash
# C√†i ƒë·∫∑t Longhorn tr√™n Cluster 2, d√πng longhorn.yaml, file ƒë√£ ch·ªânh s·ª≠a replica xu·ªëng c√≤n 2
kubectl apply -f longhorn.yaml
# ƒê·ª£i Longhorn ready
kubectl get pods -n longhorn-system --watch
### 2. Label Nodes & Create Namespaces
```bash
# Label nodes
kubectl label nodes worker3 role=monitoring
# T·∫°o namespace
kubectl create namespace monitoring
# Verify
kubectl get nodes --show-labels | grep role
````

### 3. Deploy Kube-State-Metrics

```bash
kubectl apply -f deployment/monitoring/kube-state-metrics.yaml
# Verify
kubectl get pods -n monitoring
```

### 4. Deploy Prometheus

```bash
# Deploy Prometheus v·ªõi cross-cluster scrape configs
kubectl apply -f deployment/monitoring/prometheus.yaml
# Verify
kubectl get pods -n monitoring
kubectl get svc -n monitoring
```

**Prometheus UI**: http://<cluster2-ip>:30000

### 5. Deploy Grafana

```bash
kubectl apply -f deployment/monitoring/grafana.yaml
# Verify
kubectl get pods -n monitoring
kubectl get pvc -n monitoring
```

**Grafana UI**: http://<cluster2-ip>:32000

- Username: `admin`
- Password: `admin`

---

## üîß CONFIGURE GRAFANA

### 1. Login Grafana

Truy c·∫≠p: http://<cluster2-ip>:32000

- Username: `admin`
- Password: `admin` (s·∫Ω y√™u c·∫ßu ƒë·ªïi password)

### Config grafana -> Tuy·ªÅn

## ‚úÖ KI·ªÇM TRA & TEST H·ªÜ TH·ªêNG

```bash
#thay prod b·∫±ng t√™n c√°c namespace mu·ªën xem
kubectl get svc -n prod -o wide
#kq c√¢u l·ªánh tr√™n:
```

stats-service LoadBalancer 10.43.74.177 192.168.40.202 8082:31082/TCP 42h app=stats-service
todo-frontend-service LoadBalancer 10.43.169.194 192.168.40.203 80:31891/TCP 49s app=todo-frontend-prod
todo-service LoadBalancer 10.43.66.92 192.168.40.200 8080:32033/TCP 50m app=todo-service
user-service LoadBalancer 10.43.240.91 192.168.40.201 8081:31081/TCP 27h app=user-service

```
#truy c·∫≠p frontend b·∫±ng 192.168.40.203/todo
```

```bash
#test scaling. truy c·∫≠p master2 ch·∫°y l·ªánh sau ƒë·ªÉ tƒÉng traffic cho 1 service
hey -z 5000 -c 50 http://192.168.40.200:8080/todos
#sau ƒë√≥ ch·∫°y l·ªánh sau v√† xem replica c·ªßa todo-service c√≥ tƒÉng l√™n kh√¥ng
kubectl get hpa -n prod
```

####
