# ğŸš€ HÆ¯á»šNG DáºªN DEPLOY Há»† THá»NG 2-CLUSTER

## Kiáº¿n trÃºc há»‡ thá»‘ng

**Cluster 1 (Application Cluster):**

- 1 master + 2 workers (role=app)
- Cháº¡y: Longhorn, Databases, Application services
  **Cluster 2 (Monitoring Cluster):**
- 1 master + 1 worker (role=monitoring)
- Cháº¡y: Longhorn, Prometheus, Grafana, Kube-state-metrics
- Cross-cluster metrics collection tá»« Cluster 1

## YÃªu cáº§u

- âœ… 2 K3s clusters Ä‘Ã£ cÃ i Ä‘áº·t
- âœ… kubectl cÃ³ quyá»n truy cáº­p cáº£ 2 clusters
- âœ… Docker images Ä‘Ã£ push lÃªn Docker Hub
- âœ… 2 clusters cÃ¹ng subnet

---

## ğŸ“¦ CLUSTER 1: APPLICATION CLUSTER

### 1. CÃ i Ä‘áº·t Longhorn Storage

```bash
# CÃ i Ä‘áº·t Longhorn trÃªn Cluster 1
kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.10.1/deploy/longhorn.yaml
# Äá»£i Longhorn ready
kubectl get pods -n longhorn-system --watch
# (Optional) Expose Longhorn UI
kubectl patch svc longhorn-frontend -n longhorn-system -p '{"spec":{"type":"NodePort","ports":[{"port":80,"targetPort":8000,"nodePort":30880}]}}'
```

**Longhorn UI**: http://192.168.40.121:30880

### 2. Label Nodes & Create Namespaces

```bash
# Label nodes theo role
kubectl label nodes <name_of_worker1> role=app
kubectl label nodes <name_of_worker2> role=app
# Táº¡o namespaces
kubectl create namespace databases
kubectl create namespace prod
kubectl create namespace monitoring
# Verify
kubectl get nodes --show-labels | grep role
kubectl get namespaces
```

### 3. Deploy Databases

```bash
kubectl apply -f deployment/databases/databases.yaml
# Äá»£i databases ready
kubectl get pods -n databases --watch
# Chá»: postgres-0 1/1 Running, user-db-0 1/1 Running
```

### 4. Deploy Application Services

````bash
# Deploy config vÃ  backend services
kubectl apply -f deployment/prod/config-prod.yaml
kubectl apply -f deployment/prod/application/todo-service-prod.yaml
kubectl apply -f deployment/prod/application/user-service-prod.yaml
kubectl apply -f deployment/prod/application/stats-service-prod.yaml
# Deploy frontend
kubectl apply -f deployment/prod/frontend-prod.yaml
# Kiá»ƒm tra
kubectl get pods -n prod --watch```
### 5. Expose Metrics Endpoints
```bash
# Deploy kube-state-metrics trÃªn Cluster 1
kubectl apply -f deployment/prod/kube-state-metrics.yaml
# Verify
kubectl get svc -n prod | grep metrics
kubectl get svc -n monitoring
````

**CÃ i Ä‘áº·t VIP (Virtual IP) Ä‘á»ƒ cÃ¡c services cÃ³ 1 ip chung**

```bash
kubectl apply -f deployment/prod/rbac.yaml
kubectl apply -f deployment/prod/kube-vip-daemonset.yaml
```

**CÃ¡c services Ä‘Æ°á»£c Ä‘á»•i thÃ nh LoadBalacer vÃ  cÃ³ LoadbalancerIP rá»“i -> xem chi tiáº¿t trong cÃ¡c file yaml cá»§a services**
**Truy cáº­p: 192.168.40.205**

---

### Cáº¥u hÃ¬nh HPA (Horizontal Pod Autoscaler)

**CÃ i metrics server**

```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

kubectl patch deployment metrics-server -n kube-system --type='json' -p='[{"op":"add","path":"/spec/template/spec/containers/0/args/-","value":"--kubelet-insecure-tls"}]'

kubectl get deployment metrics-server -n kube-system
```

**Ã¡p dá»¥ng hpa controller**

```bash
kubectl deployment/prod/application/hpa.yaml
kubectl get hpa -n prod
```

## ğŸ“¦ CLUSTER 2: MONITORING CLUSTER

### 1. CÃ i Ä‘áº·t Longhorn Storage

````bash
# CÃ i Ä‘áº·t Longhorn trÃªn Cluster 2, dÃ¹ng longhorn.yaml, file Ä‘Ã£ chá»‰nh sá»­a replica xuá»‘ng cÃ²n 2
kubectl apply -f longhorn.yaml
# Äá»£i Longhorn ready
kubectl get pods -n longhorn-system --watch
### 2. Label Nodes & Create Namespaces
```bash
# Label nodes
kubectl label nodes worker3 role=monitoring
# Táº¡o namespace
kubectl create namespace monitoring
# Verify
kubectl get nodes --show-labels | grep role
````

### 3. Deploy Kube-State-Metrics

```bash
kubectl apply -f deployment/monitoring/kube-state-metrics.yaml
# Verify
kubectl get pods -n monitoring
```

### 4. Deploy Prometheus

```bash
# Deploy Prometheus vá»›i cross-cluster scrape configs
kubectl apply -f deployment/monitoring/prometheus.yaml
# Verify
kubectl get pods -n monitoring
kubectl get svc -n monitoring
```

**Prometheus UI**: http://<cluster2-ip>:30000

### 5. Deploy Grafana

```bash
kubectl apply -f deployment/monitoring/grafana.yaml
# Verify
kubectl get pods -n monitoring
kubectl get pvc -n monitoring
```

**Grafana UI**: http://<cluster2-ip>:32000

- Username: `admin`
- Password: `admin`

---

## ğŸ”§ CONFIGURE GRAFANA

### 1. Login Grafana

Truy cáº­p: http://<cluster2-ip>:32000

- Username: `admin`
- Password: `admin` (sáº½ yÃªu cáº§u Ä‘á»•i password)

### Config grafana -> Tuyá»n

## âœ… KIá»‚M TRA & TEST Há»† THá»NG

```bash
#thay prod báº±ng tÃªn cÃ¡c namespace muá»‘n xem
kubectl get svc -n prod -o wide
#kq cÃ¢u lá»‡nh trÃªn:
```

stats-service LoadBalancer 10.43.74.177 192.168.40.202 8082:31082/TCP 42h app=stats-service
todo-frontend-service LoadBalancer 10.43.169.194 192.168.40.203 80:31891/TCP 49s app=todo-frontend-prod
todo-service LoadBalancer 10.43.66.92 192.168.40.200 8080:32033/TCP 50m app=todo-service
user-service LoadBalancer 10.43.240.91 192.168.40.201 8081:31081/TCP 27h app=user-service

```
#truy cáº­p frontend báº±ng 192.168.40.203/todo
```

```bash
#test scaling. truy cáº­p master2 cháº¡y lá»‡nh sau Ä‘á»ƒ tÄƒng traffic cho 1 service
hey -z 5000 -c 50 http://192.168.40.200:8080/todos
#sau Ä‘Ã³ cháº¡y lá»‡nh sau vÃ  xem replica cá»§a todo-service cÃ³ tÄƒng lÃªn khÃ´ng
kubectl get hpa -n prod
```

####
