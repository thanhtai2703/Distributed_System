# ğŸš€ HÆ¯á»šNG DáºªN DEPLOY Há»† THá»NG 2-CLUSTER

## Kiáº¿n trÃºc há»‡ thá»‘ng

**Cluster 1 (Application Cluster):**

- 1 master + 2 workers (role=app)
- Cháº¡y: Longhorn, Databases, Application services
- IP: 192.168.40.121

**Cluster 2 (Monitoring Cluster):**

- 1 master + 1 worker (role=monitoring)
- Cháº¡y: Longhorn, Prometheus, Grafana, Kube-state-metrics
- Cross-cluster metrics collection tá»« Cluster 1

## YÃªu cáº§u

- âœ… 2 K3s clusters Ä‘Ã£ cÃ i Ä‘áº·t
- âœ… kubectl cÃ³ quyá»n truy cáº­p cáº£ 2 clusters
- âœ… Docker images Ä‘Ã£ push lÃªn Docker Hub
- âœ… 2 clusters cÃ¹ng subnet hoáº·c cÃ³ káº¿t ná»‘i máº¡ng

---

## ğŸ“¦ CLUSTER 1: APPLICATION CLUSTER

### 1. CÃ i Ä‘áº·t Longhorn Storage

```bash
# CÃ i Ä‘áº·t Longhorn trÃªn Cluster 1
kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.10.1/deploy/longhorn.yaml

# Äá»£i Longhorn ready
kubectl get pods -n longhorn-system --watch

# (Optional) Expose Longhorn UI
kubectl patch svc longhorn-frontend -n longhorn-system -p '{"spec":{"type":"NodePort","ports":[{"port":80,"targetPort":8000,"nodePort":30880}]}}'
```

**Longhorn UI**: http://192.168.40.121:30880

### 2. Label Nodes & Create Namespaces

```bash
# Label nodes theo role
kubectl label nodes worker1 role=app
kubectl label nodes worker2 role=app

# Táº¡o namespaces
kubectl create namespace databases
kubectl create namespace prod
kubectl create namespace monitoring

# Verify
kubectl get nodes --show-labels | grep role
kubectl get namespaces
```

### 3. Deploy Databases

```bash
kubectl apply -f deployment/databases/databases.yaml
kubectl apply -f deployment/databases/db-services.yaml

# Äá»£i databases ready
kubectl get pods -n databases --watch
# Chá»: postgres-0 1/1 Running, user-db-0 1/1 Running
```

### 4. Deploy Application Services

```bash
# Deploy config vÃ  backend services
kubectl apply -f deployment/prod/config-prod.yaml
kubectl apply -f deployment/prod/todo-service-prod.yaml
kubectl apply -f deployment/prod/user-service-prod.yaml
kubectl apply -f deployment/prod/stats-service-prod.yaml

# Deploy frontend
kubectl apply -f deployment/prod/frontend-prod.yaml

# Kiá»ƒm tra
kubectl get pods -n prod --watch
# Chá»: 8/8 pods Running (2 replicas Ã— 3 services + 2 frontend)
```

### 5. Expose Metrics Endpoints

```bash
# Deploy NodePort services Ä‘á»ƒ expose metrics cho Cluster 2
kubectl apply -f deployment/monitoring/metrics-nodeport.yaml

# Deploy kube-state-metrics trÃªn Cluster 1
kubectl apply -f deployment/monitoring/kube-state-metrics.yaml

# Verify
kubectl get svc -n prod | grep metrics
kubectl get svc -n monitoring
```

**Metrics endpoints:**

- todo-service: `192.168.40.121:31180`
- user-service: `192.168.40.121:31181`
- stats-service: `192.168.40.121:31182`
- kube-state-metrics: `192.168.40.121:31280`

---

## ğŸ“¦ CLUSTER 2: MONITORING CLUSTER

### 1. CÃ i Ä‘áº·t Longhorn Storage

```bash
# CÃ i Ä‘áº·t Longhorn trÃªn Cluster 2
kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.10.1/deploy/longhorn.yaml

# Äá»£i Longhorn ready
kubectl get pods -n longhorn-system --watch

# QUAN TRá»ŒNG: Set replica count = 2 (vÃ¬ chá»‰ cÃ³ 2 nodes)
kubectl patch settings.longhorn.io default-replica-count -n longhorn-system --type='json' -p='[{"op": "replace", "path": "/value", "value": "2"}]'
```

### 2. Label Nodes & Create Namespaces

```bash
# Label nodes
kubectl label nodes worker1 role=monitoring

# Táº¡o namespace
kubectl create namespace monitoring

# Verify
kubectl get nodes --show-labels | grep role
```

### 3. Deploy Kube-State-Metrics

```bash
kubectl apply -f deployment/monitoring/kube-state-metrics.yaml

# Verify
kubectl get pods -n monitoring
```

### 4. Deploy Prometheus

```bash
# Deploy Prometheus vá»›i cross-cluster scrape configs
kubectl apply -f deployment/monitoring/prometheus.yaml

# Verify
kubectl get pods -n monitoring
kubectl get svc -n monitoring
```

**Prometheus UI**: http://<cluster2-ip>:30000

### 5. Deploy Grafana

```bash
kubectl apply -f deployment/monitoring/grafana.yaml

# Verify
kubectl get pods -n monitoring
kubectl get pvc -n monitoring
```

**Grafana UI**: http://<cluster2-ip>:32000

- Username: `admin`
- Password: `admin`

---

## ğŸ”§ CONFIGURE GRAFANA

### 1. Login Grafana

Truy cáº­p: http://<cluster2-ip>:32000

- Username: `admin`
- Password: `admin` (sáº½ yÃªu cáº§u Ä‘á»•i password)

### 2. Add Prometheus Datasource

- Click **âš™ï¸ Configuration** > **Data Sources** > **Add data source**
- Chá»n **Prometheus**
- URL: `http://prometheus-service.monitoring.svc:9090`
- Click **Save & Test**

### 3. Import Dashboards

**Kubernetes Cluster Monitoring:**

- Dashboard ID: **15661**

**Spring Boot Statistics:**

- Dashboard ID: **12900** hoáº·c **11378**

**JVM Micrometer:**

- Dashboard ID: **4701**

---

## ğŸŒ TRUY Cáº¬P Há»† THá»NG

### Cluster 1 - Application

- **Frontend**: http://192.168.40.121:31000
- **Todo API**: http://192.168.40.121:31080/todos
- **User API**: http://192.168.40.121:31081/users
- **Stats API**: http://192.168.40.121:31082/stats
- **Longhorn UI**: http://192.168.40.121:30880

### Cluster 2 - Monitoring

- **Prometheus**: http://<cluster2-ip>:30000
- **Grafana**: http://<cluster2-ip>:32000

---

## âœ… KIá»‚M TRA & TEST Há»† THá»NG

### 1. Verify Cluster 1

```bash
# Check pods
kubectl get pods -n prod
kubectl get pods -n databases

# Check services
kubectl get svc -n prod
kubectl get svc -n monitoring

# Test application
curl http://192.168.40.121:31000
curl http://192.168.40.121:31080/todos/actuator/health
curl http://192.168.40.121:31081/users/actuator/health
curl http://192.168.40.121:31082/stats/actuator/health

# Test metrics endpoints
curl http://192.168.40.121:31180/actuator/prometheus
curl http://192.168.40.121:31181/actuator/prometheus
curl http://192.168.40.121:31182/actuator/prometheus
```

### 2. Verify Cluster 2

```bash
# Check pods
kubectl get pods -n monitoring

# Check PVCs
kubectl get pvc -n monitoring
```

### 3. Verify Prometheus Metrics Collection

Truy cáº­p: http://<cluster2-ip>:30000

**Check Targets (Status > Targets):**

- Táº¥t cáº£ targets pháº£i **UP**

**Test Queries:**

```promql
# Services health
up{cluster="cluster1"}

# HTTP request rate
rate(http_server_requests_seconds_count{cluster="cluster1"}[5m])

# Pod status
kube_pod_status_phase{cluster="cluster1"}

# JVM memory
jvm_memory_used_bytes{cluster="cluster1"}
```

### 4. Generate Load Test

```bash
# Generate traffic to Cluster 1
for i in {1..100}; do curl -s http://192.168.40.121:31080/todos > /dev/null; done
for i in {1..100}; do curl -s http://192.168.40.121:31081/users > /dev/null; done
for i in {1..100}; do curl -s http://192.168.40.121:31082/stats > /dev/null; done

# Verify metrics in Grafana dashboards
```

---

## ğŸ”§ TROUBLESHOOTING

### Pod khÃ´ng start

```bash
kubectl describe pod <pod-name> -n <namespace>
kubectl logs <pod-name> -n <namespace>
```

### Metrics 404 Not Found

```bash
# Rebuild images vá»›i actuator dependencies
# ThÃªm vÃ o pom.xml:
# <dependency>
#   <groupId>io.micrometer</groupId>
#   <artifactId>micrometer-registry-prometheus</artifactId>
# </dependency>

# Rebuild vÃ  push
docker build -t <image>:<version> .
docker push <image>:<version>

# Update deployment
kubectl set image deployment/<name> <container>=<image>:<version> -n prod
```

### Prometheus targets DOWN

```bash
# Check metrics endpoints tá»« Cluster 2
curl -v http://192.168.40.121:31180/actuator/prometheus

# Check network connectivity
ping 192.168.40.121

# Check NodePort services
kubectl get svc -n prod | grep metrics
```

### Grafana data source error

```bash
# Verify Prometheus service
kubectl get svc prometheus-service -n monitoring

# URL pháº£i lÃ : http://prometheus-service.monitoring.svc:9090
# KhÃ´ng pháº£i: http://prometheus-service.monitoring.svc:8080
```

### Longhorn volume faulted

```bash
# Check replica count settings
kubectl get settings.longhorn.io default-replica-count -n longhorn-system -o yaml

# Cluster 2 chá»‰ cÃ³ 2 nodes, pháº£i set replica=2
kubectl patch settings.longhorn.io default-replica-count -n longhorn-system --type='json' -p='[{"op": "replace", "path": "/value", "value": "2"}]'

# Clean reinstall náº¿u cáº§n
kubectl delete namespace longhorn-system
# XÃ³a /var/lib/longhorn/* trÃªn táº¥t cáº£ nodes
# CÃ i láº¡i Longhorn
```

### Database connection error

```bash
# Check database pods
kubectl get pods -n databases

# Check logs
kubectl logs postgres-0 -n databases
kubectl logs user-db-0 -n databases
```

### Restart deployment

```bash
kubectl rollout restart deployment <deployment-name> -n <namespace>
kubectl rollout status deployment <deployment-name> -n <namespace>
```

---

## ğŸ“Š DATA RETENTION & BACKUP

### Prometheus Data Retention

Máº·c Ä‘á»‹nh: 12 giá» (cáº¥u hÃ¬nh trong `prometheus.yaml`)

**TÄƒng retention:**

```yaml
args:
  - "--storage.tsdb.retention.time=30d" # Giá»¯ 30 ngÃ y
  - "--storage.tsdb.retention.size=50GB" # Hoáº·c giá»›i háº¡n theo size
```

### Persistent Data

**Khi monitoring pods restart/die:**

- âœ… Prometheus metrics data: ÄÆ°á»£c giá»¯ láº¡i trong PVC (Longhorn volume)
- âœ… Grafana dashboards/datasources: ÄÆ°á»£c giá»¯ láº¡i trong PVC
- âœ… Database data: ÄÆ°á»£c giá»¯ láº¡i trong PVC

**Longhorn replication:**

- Cluster 1: 3 replicas (trÃªn 3 nodes)
- Cluster 2: 2 replicas (trÃªn 2 nodes)

**Test data persistence:**

```bash
# XÃ³a pods Ä‘á»ƒ giáº£ láº­p crash
kubectl delete pod -l app=prometheus-server -n monitoring
kubectl delete pod -l app=grafana -n monitoring

# Pods má»›i sáº½ mount láº¡i cÃ¹ng volume
kubectl get pods -n monitoring -w

# Verify: Dashboards vÃ  historical data váº«n cÃ²n
```

---

## ğŸ¯ KIáº¾N TRÃšC Tá»”NG QUAN

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       CLUSTER 1 (Application)           â”‚
â”‚  192.168.40.121                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Longhorn Storage (3 replicas)â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Databases                     â”‚      â”‚
â”‚  â”‚ - postgres (todo_db)          â”‚      â”‚
â”‚  â”‚ - user-db                     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Application Services          â”‚      â”‚
â”‚  â”‚ - todo-service (2 replicas)   â”‚      â”‚
â”‚  â”‚ - user-service (2 replicas)   â”‚      â”‚
â”‚  â”‚ - stats-service (2 replicas)  â”‚      â”‚
â”‚  â”‚ - frontend (2 replicas)       â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Metrics Exposure              â”‚      â”‚
â”‚  â”‚ NodePort: 31180/31181/31182   â”‚      â”‚
â”‚  â”‚ kube-state-metrics: 31280     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ Cross-cluster
              â”‚ metrics scraping
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       CLUSTER 2 (Monitoring)            â”‚
â”‚  <cluster2-ip>                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Longhorn Storage (2 replicas)â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Prometheus (Port 30000)       â”‚      â”‚
â”‚  â”‚ - Scrapes Cluster 1 metrics   â”‚      â”‚
â”‚  â”‚ - Retention: 12h              â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Grafana (Port 32000)          â”‚      â”‚
â”‚  â”‚ - Dashboards                  â”‚      â”‚
â”‚  â”‚ - Visualization               â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Kube-State-Metrics (Local)    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---
