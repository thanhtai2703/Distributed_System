# ğŸš€ HÆ¯á»šNG DáºªN DEPLOY Há»† THá»NG 2-CLUSTER

## Kiáº¿n trÃºc há»‡ thá»‘ng

**Cluster 1 (Application Cluster):**

- 1 master + 2 workers (role=app)
- Cháº¡y: Longhorn, Databases, Application services
  **Cluster 2 (Monitoring Cluster):**
- 1 master + 1 worker (role=monitoring)
- Cháº¡y: Longhorn, Prometheus, Grafana, Kube-state-metrics
- Cross-cluster metrics collection tá»« Cluster 1

## YÃªu cáº§u

- âœ… 2 K3s clusters Ä‘Ã£ cÃ i Ä‘áº·t
- âœ… kubectl cÃ³ quyá»n truy cáº­p cáº£ 2 clusters
- âœ… Docker images Ä‘Ã£ push lÃªn Docker Hub
- âœ… 2 clusters cÃ¹ng subnet hoáº·c cÃ³ káº¿t ná»‘i máº¡ng

---

## ğŸ“¦ CLUSTER 1: APPLICATION CLUSTER

### 1. CÃ i Ä‘áº·t Longhorn Storage

```bash
# CÃ i Ä‘áº·t Longhorn trÃªn Cluster 1
kubectl apply -f https://raw.githubusercontent.com/longhorn/longhorn/v1.10.1/deploy/longhorn.yaml
# Äá»£i Longhorn ready
kubectl get pods -n longhorn-system --watch

# (Optional) Expose Longhorn UI
kubectl patch svc longhorn-frontend -n longhorn-system -p '{"spec":{"type":"NodePort","ports":[{"port":80,"targetPort":8000,"nodePort":30880}]}}'
```

**Longhorn UI**: http://192.168.40.121:30880

### 2. Label Nodes & Create Namespaces

```bash
# Label nodes theo role
kubectl label nodes worker1 role=app
kubectl label nodes worker2 role=app
# Táº¡o namespaces
kubectl create namespace databases
kubectl create namespace prod
kubectl create namespace monitoring
# Verify
kubectl get nodes --show-labels | grep role
kubectl get namespaces
```

### 3. Deploy Databases

```bash
kubectl apply -f deployment/databases/databases.yaml
kubectl apply -f deployment/databases/db-services.yaml

# Äá»£i databases ready
kubectl get pods -n databases --watch
# Chá»: postgres-0 1/1 Running, user-db-0 1/1 Running
```

### 4. Deploy Application Services

```bash
# Deploy config vÃ  backend services
kubectl apply -f deployment/prod/config-prod.yaml
kubectl apply -f deployment/prod/todo-service-prod.yaml
kubectl apply -f deployment/prod/user-service-prod.yaml
kubectl apply -f deployment/prod/stats-service-prod.yaml
# Deploy frontend
kubectl apply -f deployment/prod/frontend-prod.yaml
# Kiá»ƒm tra
kubectl get pods -n prod --watch
# Chá»: 8/8 pods Running (2 replicas Ã— 3 services + 2 frontend)
```

### 5. Expose Metrics Endpoints

```bash
# Deploy NodePort services Ä‘á»ƒ expose metrics cho Cluster 2
kubectl apply -f deployment/monitoring/metrics-nodeport.yaml
# Deploy kube-state-metrics trÃªn Cluster 1
kubectl apply -f deployment/monitoring/kube-state-metrics.yaml
# Verify
kubectl get svc -n prod | grep metrics
kubectl get svc -n monitoring
```

**Metrics endpoints:**

- todo-service: `192.168.40.121:31180`
- user-service: `192.168.40.121:31181`
- stats-service: `192.168.40.121:31182`
- kube-state-metrics: `192.168.40.121:31280`

---

## ğŸ“¦ CLUSTER 2: MONITORING CLUSTER

### 1. CÃ i Ä‘áº·t Longhorn Storage

````bash
# CÃ i Ä‘áº·t Longhorn trÃªn Cluster 2, dÃ¹ng longhorn.yaml, file Ä‘Ã£ chá»‰nh sá»­a replica xuá»‘ng cÃ²n 2
kubectl apply -f longhorn.yaml
# Äá»£i Longhorn ready
kubectl get pods -n longhorn-system --watch
### 2. Label Nodes & Create Namespaces
```bash
# Label nodes
kubectl label nodes worker3 role=monitoring
# Táº¡o namespace
kubectl create namespace monitoring
# Verify
kubectl get nodes --show-labels | grep role
````

### 3. Deploy Kube-State-Metrics

```bash
kubectl apply -f deployment/monitoring/kube-state-metrics.yaml
# Verify
kubectl get pods -n monitoring
```

### 4. Deploy Prometheus

```bash
# Deploy Prometheus vá»›i cross-cluster scrape configs
kubectl apply -f deployment/monitoring/prometheus.yaml
# Verify
kubectl get pods -n monitoring
kubectl get svc -n monitoring
```

**Prometheus UI**: http://<cluster2-ip>:30000

### 5. Deploy Grafana

```bash
kubectl apply -f deployment/monitoring/grafana.yaml
# Verify
kubectl get pods -n monitoring
kubectl get pvc -n monitoring
```

**Grafana UI**: http://<cluster2-ip>:32000

- Username: `admin`
- Password: `admin`

---

## ğŸ”§ CONFIGURE GRAFANA

### 1. Login Grafana

Truy cáº­p: http://<cluster2-ip>:32000

- Username: `admin`
- Password: `admin` (sáº½ yÃªu cáº§u Ä‘á»•i password)

### Config grafana -> Tuyá»n

## ğŸŒ TRUY Cáº¬P Há»† THá»NG

### Cluster 1 - Application

- **Frontend**: http://192.168.40.121:31000
- **Todo API**: http://192.168.40.121:31080/todos
- **User API**: http://192.168.40.121:31081/users
- **Stats API**: http://192.168.40.121:31082/stats
- **Longhorn UI**: http://192.168.40.121:30880

### Cluster 2 - Monitoring

- **Prometheus**: http://<cluster2-ip>:30000
- **Grafana**: http://<cluster2-ip>:32000

---

## âœ… KIá»‚M TRA & TEST Há»† THá»NG

### 1. Verify Cluster 1

```bash
# Check pods
kubectl get pods -n prod
kubectl get pods -n databases

# Check services
kubectl get svc -n prod
kubectl get svc -n monitoring

# Test application
curl http://192.168.40.121:31000
curl http://192.168.40.121:31080/todos/actuator/health
curl http://192.168.40.121:31081/users/actuator/health
curl http://192.168.40.121:31082/stats/actuator/health

# Test metrics endpoints
curl http://192.168.40.121:31180/actuator/prometheus
curl http://192.168.40.121:31181/actuator/prometheus
curl http://192.168.40.121:31182/actuator/prometheus
```

### 2. Verify Cluster 2

```bash
# Check pods
kubectl get pods -n monitoring
# Check PVCs
kubectl get pvc -n monitoring
```

### 3. Verify Prometheus Metrics Collection

Truy cáº­p: http://<cluster2-ip>:30000

**Check Targets (Status > Targets):**

- Táº¥t cáº£ targets pháº£i **UP**

**Test Queries:**

```promql
# Services health
up{cluster="cluster1"}

# HTTP request rate
rate(http_server_requests_seconds_count{cluster="cluster1"}[5m])

# Pod status
kube_pod_status_phase{cluster="cluster1"}

# JVM memory
jvm_memory_used_bytes{cluster="cluster1"}
```

---

### Grafana data source error

````bash
# Verify Prometheus service
kubectl get svc prometheus-service -n monitoring
# URL pháº£i lÃ : http://prometheus-service.monitoring.svc:9090
---

## ğŸ“Š DATA RETENTION & BACKUP

### Prometheus Data Retention

Máº·c Ä‘á»‹nh: 12 giá» (cáº¥u hÃ¬nh trong `prometheus.yaml`)

**TÄƒng retention:**

```yaml
args:
  - "--storage.tsdb.retention.time=30d" # Giá»¯ 30 ngÃ y
  - "--storage.tsdb.retention.size=50GB" # Hoáº·c giá»›i háº¡n theo size
````

### Persistent Data

**Khi monitoring pods restart/die:**

- âœ… Prometheus metrics data: ÄÆ°á»£c giá»¯ láº¡i trong PVC (Longhorn volume)
- âœ… Grafana dashboards/datasources: ÄÆ°á»£c giá»¯ láº¡i trong PVC
- âœ… Database data: ÄÆ°á»£c giá»¯ láº¡i trong PVC

**Longhorn replication:**

- Cluster 1: 3 replicas (trÃªn 3 nodes)
- Cluster 2: 2 replicas (trÃªn 2 nodes)

**Test data persistence:**

```bash
# XÃ³a pods Ä‘á»ƒ giáº£ láº­p crash
kubectl delete pod -l app=prometheus-server -n monitoring
kubectl delete pod -l app=grafana -n monitoring
# Pods má»›i sáº½ mount láº¡i cÃ¹ng volume
kubectl get pods -n monitoring -w
```

---
